{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ouassimkiassa/opt/anaconda3/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "from sklearn.feature_selection import SelectFwe, f_classif\n",
    "\n",
    "\n",
    "\n",
    "# Average CV score on the training set was: 0.86744\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        make_pipeline(\n",
    "            make_union(\n",
    "                FunctionTransformer(copy),\n",
    "                FunctionTransformer(copy)\n",
    "            ),\n",
    "            SelectFwe(score_func=f_classif, alpha=0.037)\n",
    "        )\n",
    "    ),\n",
    "    MultinomialNB(alpha=0.001, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1)\n",
    "\n",
    "exported_pipeline.fit(vectors, newsgroups_train.target)\n",
    "results = exported_pipeline.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.51      0.39       319\n",
      "           1       0.65      0.74      0.69       389\n",
      "           2       0.68      0.54      0.60       394\n",
      "           3       0.62      0.69      0.65       392\n",
      "           4       0.74      0.69      0.72       385\n",
      "           5       0.82      0.76      0.79       395\n",
      "           6       0.85      0.74      0.79       390\n",
      "           7       0.77      0.73      0.75       396\n",
      "           8       0.80      0.75      0.77       398\n",
      "           9       0.94      0.80      0.87       397\n",
      "          10       0.93      0.91      0.92       399\n",
      "          11       0.75      0.76      0.75       396\n",
      "          12       0.70      0.56      0.63       393\n",
      "          13       0.85      0.78      0.81       396\n",
      "          14       0.76      0.79      0.77       394\n",
      "          15       0.65      0.82      0.73       398\n",
      "          16       0.59      0.72      0.65       364\n",
      "          17       0.83      0.79      0.81       376\n",
      "          18       0.52      0.48      0.50       310\n",
      "          19       0.40      0.34      0.37       251\n",
      "\n",
      "    accuracy                           0.71      7532\n",
      "   macro avg       0.71      0.70      0.70      7532\n",
      "weighted avg       0.72      0.71      0.71      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(newsgroups_test.target,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162   0   1   2   2   3   0   3   2   1   2   2   1   3   9  56   7  13\n",
      "   13  37]\n",
      " [  9 287  15  12  13  24   3   0   2   0   0  10   1   0   7   2   0   1\n",
      "    1   2]\n",
      " [ 18  33 211  53   7  29   3   2   5   0   0   8   4   2  10   1   0   0\n",
      "    4   4]\n",
      " [  7  13  29 270  30   2   9   1   0   1   1   5  21   0   1   0   0   0\n",
      "    2   0]\n",
      " [ 14  11  14  30 266   2   9   5   1   0   0   6  16   2   5   2   0   0\n",
      "    1   1]\n",
      " [  6  45  15   7   4 300   3   0   0   1   0   3   4   1   3   1   1   0\n",
      "    0   1]\n",
      " [  9   2   2  26  16   0 290  12   8   1   2   1   8   0   1   3   3   0\n",
      "    5   1]\n",
      " [ 26   1   2   0   0   0   8 291  23   1   1   5   9   3   7   3   5   1\n",
      "    9   1]\n",
      " [ 19   3   1   1   2   0   4  19 298   1   0   0  10   5   6   1  10   3\n",
      "   10   5]\n",
      " [ 19   3   0   0   0   1   1   2   3 319  17   5   1   3   3   4   4   2\n",
      "    7   3]\n",
      " [ 16   0   0   0   0   1   0   0   1   4 365   1   0   1   2   3   4   0\n",
      "    0   1]\n",
      " [ 21   6   6   3   4   1   0   0   4   3   0 300   5   1   5   1  19   2\n",
      "   12   3]\n",
      " [ 12  14  10  25  14   1  11  15   8   1   0  29 221  12  12   0   1   1\n",
      "    3   3]\n",
      " [ 20   5   1   1   0   0   0   8   6   0   2   1   5 310   3  11   7   3\n",
      "   11   2]\n",
      " [ 22   7   1   1   0   0   0   6   2   1   1   2   6   5 311   4   3   7\n",
      "   11   4]\n",
      " [ 30   3   1   0   1   0   0   0   0   1   0   1   0   1   3 328   3   0\n",
      "    3  23]\n",
      " [ 18   0   1   0   0   0   1   4   3   1   0   8   0   2   9   9 261   9\n",
      "   19  19]\n",
      " [ 24   2   0   1   0   0   0   1   5   1   1   4   0   0   0  11   6 297\n",
      "   15   8]\n",
      " [ 24   2   0   0   0   0   1   5   2   0   0   4   0   9   7   4  87   8\n",
      "  148   9]\n",
      " [ 37   3   0   1   0   1   0   2   1   1   0   4   2   5   5  62  23   9\n",
      "   10  85]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(newsgroups_test.target,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7063197026022305\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(newsgroups_test.target, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "def load(path):\n",
    "    df = load_files(f\"{path}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"./datasets/\"\n",
    "train_path = f\"{ROOT_PATH}aclImdb/train/\"\n",
    "test_path = f\"{ROOT_PATH}aclImdb/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load(train_path)\n",
    "test = load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(train.data)\n",
    "vectors_test = vectorizer.transform(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "\n",
    "# Average CV score on the training set was: 0.86744\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        SelectPercentile(score_func=f_classif, percentile=6)\n",
    "    ),\n",
    "    MultinomialNB(alpha=0.1, fit_prior=False)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 1)\n",
    "\n",
    "exported_pipeline.fit(vectors, train.target)\n",
    "results = exported_pipeline.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84     12500\n",
      "           1       0.86      0.79      0.82     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test.target,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10883  1617]\n",
      " [ 2650  9850]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test.target,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82932\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test.target, results))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cca706f505208c4b21b29d5a035e2b3408c985b39f4d19b15648ac6df55340d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
